{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Observe the types of all the columns \n",
    "print(\"=\" * 50)\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "df['date'] = pd.to_datetime(df['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "if 'ticker' in df.columns:\n",
    "        df['ticker'] = df['ticker'].astype('category')\n",
    "\n",
    "# Rename window_start to unix_nsec_timestamp\n",
    "df = df.rename(columns={'window_start': 'unix_nsec_timestamp'})\n",
    "\n",
    "# Reorder columns to match desired order: date, unix_nsec_timestamp, ticker, open, close, high, low, volume, transactions\n",
    "desired_order = ['date', 'unix_nsec_timestamp', 'ticker', 'open', 'close', 'high', 'low', 'volume', 'transactions']\n",
    "df = df[desired_order]\n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "\n",
    "\n",
    "# Check for NaN values in ticker column\n",
    "print(f\"NaN count: {df['ticker'].isna().sum()}\")\n",
    "print(f\"NaN percentage: {df['ticker'].isna().sum() / len(df) * 100:.2f}%\")\n",
    "print(f\"Unique tickers (with NaN): {len(df['ticker'].unique())}\")\n",
    "print(f\"Unique tickers (without NaN): {len(df['ticker'].dropna().unique())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c128248",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "df['date'] = pd.to_datetime(df['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "def build_payload(\n",
    "\tdf: pd.DataFrame,\n",
    "\tthreshold: int\n",
    "\t) -> List[Tuple[str, date]]:\n",
    "\t\"\"\" \n",
    "\tBuilds the data that we want to send to the API\n",
    "\n",
    "\tWe will be sending a ticker and date, \n",
    "\t\"\"\"\n",
    "\t\n",
    "\t# Get each unique ticker with its first and last date\n",
    "\tticker_dates = df.groupby('ticker').agg(\n",
    "\t\tfirst_date=('date', 'min'),\n",
    "\t\tlast_date=('date', 'max')\n",
    "\t).reset_index()\n",
    "\n",
    "\t# Generate list of (ticker, date) tuples for each ticker from first_date to first_date + threshold\n",
    "\tlist_of_calls = []\n",
    "\n",
    "\t# Now fill in dates for each ticker group with the threshold we choose\n",
    "\tfor _, row in ticker_dates.iterrows():\n",
    "\t\tticker = row['ticker']\n",
    "\t\tfirst_date = pd.to_datetime(row['first_date'])\n",
    "\t\tlast_date = pd.to_datetime(row['last_date'])\n",
    "\t\t\n",
    "\t\t# Generate dates from first_date to end_date, with threshold as the interval\n",
    "\t\tdate_range = pd.date_range(\n",
    "\t\t\tstart=first_date,\n",
    "\t\t\tend=last_date,\n",
    "\t\t\tfreq=f'{threshold}D'\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\t# Add (ticker, date) tuple for each date\n",
    "\t\tfor date in date_range:\n",
    "\t\t\tlist_of_calls.append((ticker, date.date()))\n",
    "\t\n",
    "\treturn list_of_calls\n",
    "\n",
    "list = build_payload(df, 14)\n",
    "\n",
    "for item in list:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of tickers that have very little continuous trading data and determine a threshold to filter out, perhaps less than 30 days\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "\n",
    "tickers_too_little_data = []\n",
    "\n",
    "for ticker, group in df.groupby('ticker', observed=True):\n",
    "    dates = group['date'].unique()\n",
    "\n",
    "    if len(dates) < 30:\n",
    "        tickers_too_little_data.append(ticker)\n",
    "\n",
    "print(len(tickers_too_little_data))\n",
    "print(tickers_too_little_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d531882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Test: String (object) vs Category dtype for ticker column\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df_test = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date\n",
    "df_test['date'] = pd.to_datetime(df_test['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in df_test.columns if col != 'date']\n",
    "df_test = df_test[cols]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET INFO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total rows: {len(df_test):,}\")\n",
    "print(f\"Unique tickers: {df_test['ticker'].nunique():,}\")\n",
    "print(f\"Average rows per ticker: {len(df_test) / df_test['ticker'].nunique():.1f}\")\n",
    "print(f\"\\nMemory usage (object dtype):\")\n",
    "mem_obj = df_test['ticker'].memory_usage(deep=True) / 1024**2\n",
    "print(f\"{mem_obj:.2f} MB\")\n",
    "\n",
    "# Create a copy with category dtype\n",
    "df_cat = df_test.copy()\n",
    "df_cat['ticker'] = df_cat['ticker'].astype('category')\n",
    "\n",
    "print(f\"\\nMemory usage (category dtype):\")\n",
    "mem_cat = df_cat['ticker'].memory_usage(deep=True) / 1024**2\n",
    "print(f\"{mem_cat:.2f} MB\")\n",
    "print(f\"Memory savings: {(1 - mem_cat / mem_obj) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: groupby operations\n",
    "print(\"\\n1. GROUPBY OPERATIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "result_obj = df_test.groupby('ticker')['volume'].sum()\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "result_cat = df_cat.groupby('ticker')['volume'].sum()\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 2: Filtering with isin()\n",
    "print(\"\\n2. FILTERING WITH isin()\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Get a sample of tickers to filter\n",
    "sample_tickers = df_test['ticker'].unique()[:1000].tolist()\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "filtered_obj = df_test[df_test['ticker'].isin(sample_tickers)]\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "filtered_cat = df_cat[df_cat['ticker'].isin(sample_tickers)]\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 3: Sorting\n",
    "print(\"\\n3. SORTING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "sorted_obj = df_test.sort_values('ticker')\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "sorted_cat = df_cat.sort_values('ticker')\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 4: value_counts()\n",
    "print(\"\\n4. value_counts()\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "vc_obj = df_test['ticker'].value_counts()\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "vc_cat = df_cat['ticker'].value_counts()\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 5: set_index with ticker\n",
    "print(\"\\n5. set_index() with ticker\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "idx_obj = df_test.set_index(['ticker', 'date'])\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "idx_cat = df_cat.set_index(['ticker', 'date'])\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 6: Aggregation operations\n",
    "print(\"\\n6. MULTIPLE AGGREGATIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "agg_obj = df_test.groupby('ticker').agg({\n",
    "    'volume': ['sum', 'mean', 'max'],\n",
    "    'close': ['mean', 'std']\n",
    "})\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "agg_cat = df_cat.groupby('ticker').agg({\n",
    "    'volume': ['sum', 'mean', 'max'],\n",
    "    'close': ['mean', 'std']\n",
    "})\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ba263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing how much of the data (or haw many ticker series) have at MAX 100 shares vol traded on a day through their ENTIRE series \n",
    "# and never reached above $0.01 on their ENTIRE series. \n",
    "\n",
    "# Goal is to potentially remove some untradeable noise, but first to see how much of that noise we would be removing\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "unique_tickers = OHLCV_data['ticker'].unique()\n",
    "\n",
    "# Find and analyze the number of tickers that have MAX 100 shares volume across their entire series \n",
    "# Compute max volume and max price per ticker in one pass\n",
    "ticker_stats = OHLCV_data.groupby('ticker').agg({\n",
    "    'volume': 'max',\n",
    "    'close': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "# See what the distribution of price and volume is across our series\n",
    "print(ticker_stats['volume'].min())\n",
    "print(ticker_stats['volume'].quantile([0.01, 0.05, 0.10, 0.25, 0.50]))\n",
    "print(ticker_stats['close'].min())\n",
    "\n",
    "# Filter tickers into a list that are below the criteria\n",
    "invalid_tickers = ticker_stats[\n",
    "    (ticker_stats['volume'] < 1000) |\n",
    "    (ticker_stats['close'] < 0.01)\n",
    "]['ticker'].tolist()\n",
    "\n",
    "print(invalid_tickers)\n",
    "print(len(invalid_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0999a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all of the suffixes we have for the unique tickers\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "# Get unique tickers and filter out NaN values, convert to strings\n",
    "unique_tickers = OHLCV_data['ticker'].dropna().unique()\n",
    "unique_tickers = [str(ticker) for ticker in unique_tickers]\n",
    "\n",
    "# Function to extract suffix from a ticker\n",
    "def extract_suffix(ticker):\n",
    "    \"\"\"\n",
    "    Extract suffix from ticker. Returns (base, suffix) or (ticker, None) if no suffix detected.\n",
    "    Suffixes are typically 1-3 characters at the end that are not digits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Match test tickers (case-insensitive)\n",
    "    if re.search(r'(?i)test', ticker):\n",
    "        # Handle test tickers\n",
    "        return ticker, 'TEST'\n",
    "\n",
    "    # Match ZVZZT/ZWZZT test tickers\n",
    "    if re.match(r'^(ZVZZT|ZWZZT)$', ticker):\n",
    "        return ticker, None\n",
    "\n",
    "    # Match non-equities (lowercase/period suffixes)\n",
    "    non_equities_match = re.match(r'^([^a-z.]*)([a-z.].*)$', ticker)\n",
    "    if non_equities_match:\n",
    "        base, suffix = non_equities_match.groups()\n",
    "        if len(base) >= 1 and len(suffix) >= 1:\n",
    "            return base, suffix\n",
    "\n",
    "    # If no pattern matches, return the whole ticker as base with no suffix\n",
    "    return ticker, None\n",
    "\n",
    "# Group tickers by suffix - simple dictionary mapping suffix -> list of tickers\n",
    "suffix_groups = defaultdict(list)\n",
    "\n",
    "for ticker in unique_tickers:\n",
    "    base, suffix = extract_suffix(ticker)\n",
    "    if suffix:\n",
    "        suffix_groups[suffix].append(ticker)\n",
    "    else:\n",
    "        # Tickers with no detected suffix go under a no_suffix column\n",
    "        suffix_groups['no suffix'].append(ticker)\n",
    "\n",
    "\n",
    "# Find the maximum length needed for DataFrame\n",
    "max_len = max(len(tickers) for tickers in suffix_groups.values()) if suffix_groups else 0\n",
    "\n",
    "# Pad all lists to the same length with NaN\n",
    "for suffix in suffix_groups:\n",
    "    suffix_groups[suffix] = suffix_groups[suffix] + [np.nan] * (max_len - len(suffix_groups[suffix]))\n",
    "\n",
    "# Create DataFrame\n",
    "suffix_df = pd.DataFrame(dict(sorted(suffix_groups.items())))\n",
    "\n",
    "# take a look at the dataframe\n",
    "print(suffix_df)\n",
    "\n",
    "# Dictionary to store the column names (suffixes) and the COUNT of tickers that fall under that suffix \n",
    "# we dont currently use the dictionary but could be nice to have\n",
    "suffix_dict = {}\n",
    "\n",
    "for column_name, column_data in suffix_df.items():\n",
    "    suffix_dict[column_name] = column_data.count()\n",
    "    print(f\"{column_name} : {column_data.count()}\")\n",
    "\n",
    "# number of non suffixed unique tickers\n",
    "count_no_suffix = suffix_dict['no suffix']\n",
    "# number of uniquely suffix tickers identified by the regex\n",
    "count_suffix = len(unique_tickers) - count_no_suffix\n",
    "\n",
    "# Percent of uniquely suffixed tickers for all unique tickers \n",
    "# Will be removinig these \n",
    "print(count_suffix / len(unique_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22290f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also remove rights, warrants and units confidently by checking for 5 lengthed tickers with U, W, or R as their 5th char\n",
    "# that have a 4 char lengthed ticker, \n",
    "# we dont want to try and simply remove tickers based on them having a U, R, W at the end of their string, but if \n",
    "# there is another ticker that is an exact match up to the final character, and the final character is a W, U or R\n",
    "# we can confidently remove those tickers, keeping on the vanilla equity.\n",
    "\n",
    "# Look at all of the suffixes we have for the unique tickers\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "df['date'] = pd.to_datetime(df['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in df.columns if col != 'date']\n",
    "df[cols]\n",
    "\n",
    "\n",
    "def sanitize_warrants_rights_units(\n",
    "    df: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removes warrants, rights, and units by checking for 5-character tickers ending in U, W, or R\n",
    "    that have a matching 4-character base ticker.\n",
    "    \"\"\"\n",
    "    unique_tickers = set(df['ticker'].dropna().unique())\n",
    "    tickers_to_remove = []\n",
    "\n",
    "    for ticker in unique_tickers:\n",
    "        if isinstance(ticker, str) and len(ticker) == 5 and ticker[4] in ['U', 'W', 'R']:\n",
    "            base_ticker = ticker[:4]\n",
    "            if base_ticker in unique_tickers:\n",
    "                tickers_to_remove.append(ticker)\n",
    "\n",
    "    mask_to_keep = ~df['ticker'].isin(tickers_to_remove)\n",
    "    return df[mask_to_keep], tickers_to_remove\n",
    "\n",
    "df, tickers_to_remove  = sanitize_warrants_rights_units(OHLCV_data)\n",
    "\n",
    "print(len(tickers_to_remove))\n",
    "for ticker in tickers_to_remove:\n",
    "    print(ticker)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ceb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing duplicates\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "# Find the duplcate rows\n",
    "duplicate_rows = OHLCV_data[OHLCV_data.duplicated(subset=['ticker', 'date'], keep=False)]\n",
    "print(type(duplicate_rows))\n",
    "\n",
    "# Select only 'ticker' and 'date' columns, sort by ticker\n",
    "dupes = duplicate_rows[['ticker', 'date', 'close','open', 'volume']].sort_values(by='ticker')\n",
    "\n",
    "print(dupes)\n",
    "print(len(dupes))\n",
    "# print(len(duplicate_rows))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (emerytrading)",
   "language": "python",
   "name": "emerytrading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
