{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "# print(OHLCV_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ba263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing how much of the data (or haw many ticker series) have at MAX 100 shares vol traded on a day through their ENTIRE series \n",
    "# and never reached above $0.01 on their ENTIRE series. \n",
    "\n",
    "# Goal is to potentially remove some untradeable noise, but first to see how much of that noise we would be removing\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "unique_tickers = OHLCV_data['ticker'].unique()\n",
    "\n",
    "# Find and analyze the number of tickers that have MAX 100 shares volume across their entire series \n",
    "# Compute max volume and max price per ticker in one pass\n",
    "ticker_stats = OHLCV_data.groupby('ticker').agg({\n",
    "    'volume': 'max',\n",
    "    'close': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "# See what the distribution of price and volume is across our series\n",
    "print(ticker_stats['volume'].min())\n",
    "print(ticker_stats['volume'].quantile([0.01, 0.05, 0.10, 0.25, 0.50]))\n",
    "print(ticker_stats['close'].min())\n",
    "\n",
    "# Filter tickers into a list that are below the criteria\n",
    "invalid_tickers = ticker_stats[\n",
    "    (ticker_stats['volume'] < 1000) |\n",
    "    (ticker_stats['close'] < 0.01)\n",
    "]['ticker'].tolist()\n",
    "\n",
    "print(invalid_tickers)\n",
    "print(len(invalid_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0999a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all of the suffixes we have for the unique tickers\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "# Get unique tickers and filter out NaN values, convert to strings\n",
    "unique_tickers = OHLCV_data['ticker'].dropna().unique()\n",
    "unique_tickers = [str(ticker) for ticker in unique_tickers]\n",
    "\n",
    "# Function to extract suffix from a ticker\n",
    "def extract_suffix(ticker):\n",
    "    \"\"\"\n",
    "    Extract suffix from ticker. Returns (base, suffix) or (ticker, None) if no suffix detected.\n",
    "    Suffixes are typically 1-3 characters at the end that are not digits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Match test tickers (case-insensitive)\n",
    "    if re.search(r'(?i)test', ticker):\n",
    "        # Handle test tickers\n",
    "        return ticker, 'TEST'\n",
    "\n",
    "    # Match ZVZZT/ZWZZT test tickers\n",
    "    if re.match(r'^(ZVZZT|ZWZZT)$', ticker):\n",
    "        return ticker, None\n",
    "\n",
    "    # Match non-equities (lowercase/period suffixes)\n",
    "    non_equities_match = re.match(r'^([^a-z.]*)([a-z.].*)$', ticker)\n",
    "    if non_equities_match:\n",
    "        base, suffix = non_equities_match.groups()\n",
    "        if len(base) >= 1 and len(suffix) >= 1:\n",
    "            return base, suffix\n",
    "\n",
    "    # If no pattern matches, return the whole ticker as base with no suffix\n",
    "    return ticker, None\n",
    "\n",
    "# Group tickers by suffix - simple dictionary mapping suffix -> list of tickers\n",
    "suffix_groups = defaultdict(list)\n",
    "\n",
    "for ticker in unique_tickers:\n",
    "    base, suffix = extract_suffix(ticker)\n",
    "    if suffix:\n",
    "        suffix_groups[suffix].append(ticker)\n",
    "    else:\n",
    "        # Tickers with no detected suffix go under a no_suffix column\n",
    "        suffix_groups['no suffix'].append(ticker)\n",
    "\n",
    "\n",
    "# Find the maximum length needed for DataFrame\n",
    "max_len = max(len(tickers) for tickers in suffix_groups.values()) if suffix_groups else 0\n",
    "\n",
    "# Pad all lists to the same length with NaN\n",
    "for suffix in suffix_groups:\n",
    "    suffix_groups[suffix] = suffix_groups[suffix] + [np.nan] * (max_len - len(suffix_groups[suffix]))\n",
    "\n",
    "# Create DataFrame\n",
    "suffix_df = pd.DataFrame(dict(sorted(suffix_groups.items())))\n",
    "\n",
    "# take a look at the dataframe\n",
    "print(suffix_df)\n",
    "\n",
    "# Dictionary to store the column names (suffixes) and the COUNT of tickers that fall under that suffix \n",
    "# we dont currently use the dictionary but could be nice to have\n",
    "suffix_dict = {}\n",
    "\n",
    "for column_name, column_data in suffix_df.items():\n",
    "    suffix_dict[column_name] = column_data.count()\n",
    "    print(f\"{column_name} : {column_data.count()}\")\n",
    "\n",
    "# number of non suffixed unique tickers\n",
    "count_no_suffix = suffix_dict['no suffix']\n",
    "# number of uniquely suffix tickers identified by the regex\n",
    "count_suffix = len(unique_tickers) - count_no_suffix\n",
    "\n",
    "# Percent of uniquely suffixed tickers for all unique tickers \n",
    "# Will be removinig these \n",
    "print(count_suffix / len(unique_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2ceb8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ticker       date     close      open  volume\n",
      "7326351   AMUB 2019-08-13   14.6595   14.6595       0\n",
      "7334826   AMUB 2019-08-13   14.7448   14.7448       1\n",
      "7326911   BNKU 2019-08-13   44.2546   44.2546    1000\n",
      "7335391   BNKU 2019-08-13   45.9859   45.9859       5\n",
      "7326988   BSCE 2019-08-13   26.1300   26.1300      10\n",
      "7335466   BSCE 2019-08-13   26.0950   26.0950      57\n",
      "7335521    BUY 2019-08-13   18.2100   18.2600     500\n",
      "7327043    BUY 2019-08-13   17.9200   17.9200       0\n",
      "7327802   DAUD 2019-08-13   36.4729   36.4729       0\n",
      "7336290   DAUD 2019-08-13   35.6398   35.5929     620\n",
      "7336818   EMSG 2019-08-13   24.5109   24.5109       0\n",
      "7328324   EMSG 2019-08-13   24.1899   24.1899       0\n",
      "7328606   FAUS 2019-08-13   30.4600   30.4600       0\n",
      "7337099   FAUS 2019-08-13   31.0524   31.0524       0\n",
      "7328778   FLEU 2019-08-13  127.5950  127.5950       0\n",
      "7337276   FLEU 2019-08-13  129.6009  129.6009     100\n",
      "7329193   GLBY 2019-08-13   27.5450   27.5450       0\n",
      "7337699   GLBY 2019-08-13   27.4950   27.4950       0\n",
      "7329582   HONR 2019-08-13   25.3813   25.3813       0\n",
      "7338085   HONR 2019-08-13   25.6648   25.6648       0\n",
      "7329903   IMFC 2019-08-13   26.3650   26.3650       0\n",
      "7338403   IMFC 2019-08-13   26.3250   26.3250    1170\n",
      "7338428   INAU 2019-08-13   27.0802   27.0802       0\n",
      "7329927   INAU 2019-08-13   26.6739   26.6739       0\n",
      "7338619   IXSE 2019-08-13   23.1025   23.1025       0\n",
      "7330114   IXSE 2019-08-13   23.1400   23.1400       0\n",
      "7338710    JJE 2019-08-13   44.0237   44.0237       0\n",
      "7330206    JJE 2019-08-13   42.4173   42.4173       0\n",
      "7340066   OLEM 2019-08-13   17.2146   17.2146       0\n",
      "7331546   OLEM 2019-08-13   16.4727   16.4727       0\n",
      "7331996   PQLC 2019-08-13   51.3207   51.3207       0\n",
      "7340512   PQLC 2019-08-13   51.9695   51.9695       0\n",
      "7340514   PQSV 2019-08-13   47.4562   47.4562       0\n",
      "7331998   PQSV 2019-08-13   47.0861   47.0861       0\n",
      "7340682   PYPE 2019-08-13   20.8633   20.8633       0\n",
      "7332169   PYPE 2019-08-13   20.7407   20.7407       0\n",
      "7340787   RBIN 2019-08-13   24.6100   24.6100     200\n",
      "7332274   RBIN 2019-08-13   24.4700   24.4700       0\n",
      "7340901   RIDV 2019-08-13   21.8930   21.8930       0\n",
      "7332388   RIDV 2019-08-13   21.6639   21.6639       0\n",
      "7341163   SCAP 2019-08-13   34.6050   34.7800     100\n",
      "7332644   SCAP 2019-08-13   34.2750   34.2750       0\n",
      "7341208   SCOM 2019-08-13   26.4030   26.4600     102\n",
      "7332690   SCOM 2019-08-13   27.5641   27.5641       1\n",
      "7333727   ULTR 2019-08-13   50.0450   50.0450       0\n",
      "7342250   ULTR 2019-08-13   50.0450   50.0450       0\n",
      "7342314   USDY 2019-08-13   25.7851   25.7851       0\n",
      "7333791   USDY 2019-08-13   25.4867   25.4867       0\n",
      "7342321    USI 2019-08-13   25.0350   25.0350       0\n",
      "7333798    USI 2019-08-13   25.0500   25.0500       0\n",
      "7342414   VEGA 2019-08-13   32.0850   32.0850       0\n",
      "7333891   VEGA 2019-08-13   31.7850   31.7850       0\n",
      "7342437   VGFO 2019-08-13   25.5788   25.6272     300\n",
      "7333913   VGFO 2019-08-13   25.2737   25.2737       0\n",
      "7334324    XMX 2019-08-13   29.2235   29.2235       0\n",
      "7342850    XMX 2019-08-13   29.5725   29.5725       0\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "# Analyzing duplicates\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "# Find the duplcate rows\n",
    "duplicate_rows = OHLCV_data[OHLCV_data.duplicated(subset=['ticker', 'date'], keep=False)]\n",
    "\n",
    "# Select only 'ticker' and 'date' columns, sort by ticker\n",
    "dupes = duplicate_rows[['ticker', 'date', 'close','open', 'volume']].sort_values(by='ticker')\n",
    "\n",
    "print(dupes)\n",
    "print(len(dupes))\n",
    "# print(len(duplicate_rows))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (emerytrading)",
   "language": "python",
   "name": "emerytrading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
