{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e50d77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Observe the types of all the columns \n",
    "print(\"=\" * 50)\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "df['date'] = pd.to_datetime(df['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "if 'ticker' in df.columns:\n",
    "        df['ticker'] = df['ticker'].astype('category')\n",
    "\n",
    "# Rename window_start to unix_nsec_timestamp\n",
    "df = df.rename(columns={'window_start': 'unix_nsec_timestamp'})\n",
    "\n",
    "# Reorder columns to match desired order: date, unix_nsec_timestamp, ticker, open, close, high, low, volume, transactions\n",
    "desired_order = ['date', 'unix_nsec_timestamp', 'ticker', 'open', 'close', 'high', 'low', 'volume', 'transactions']\n",
    "df = df[desired_order]\n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(df.head())\n",
    "print(df.dtypes)\n",
    "\n",
    "\n",
    "# Check for NaN values in ticker column\n",
    "print(f\"NaN count: {df['ticker'].isna().sum()}\")\n",
    "print(f\"NaN percentage: {df['ticker'].isna().sum() / len(df) * 100:.2f}%\")\n",
    "print(f\"Unique tickers (with NaN): {len(df['ticker'].unique())}\")\n",
    "print(f\"Unique tickers (without NaN): {len(df['ticker'].dropna().unique())}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3104341f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DISTRIBUTION OF DATA POINTS PER TICKER\n",
      "======================================================================\n",
      "\n",
      "1. NUMBER OF UNIQUE DAYS PER TICKER:\n",
      "count    23621.000000\n",
      "mean       990.949663\n",
      "std        858.446496\n",
      "min          1.000000\n",
      "25%        254.000000\n",
      "50%        695.000000\n",
      "75%       1663.000000\n",
      "max       2468.000000\n",
      "Name: num_days, dtype: float64\n",
      "\n",
      "Percentiles:\n",
      "  1th percentile: 4 days (235 tickers have fewer)\n",
      "  5th percentile: 25 days (1,172 tickers have fewer)\n",
      "  10th percentile: 88 days (2,344 tickers have fewer)\n",
      "  25th percentile: 254 days (5,899 tickers have fewer)\n",
      "  50th percentile: 695 days (11,807 tickers have fewer)\n",
      "  75th percentile: 1663 days (17,711 tickers have fewer)\n",
      "  90th percentile: 2468 days (20,399 tickers have fewer)\n",
      "  95th percentile: 2468 days (20,399 tickers have fewer)\n",
      "  99th percentile: 2468 days (20,399 tickers have fewer)\n",
      "\n",
      "2. TIME SPAN (CALENDAR DAYS FROM FIRST TO LAST):\n",
      "count    23621.000000\n",
      "mean      1547.307989\n",
      "std       1245.705514\n",
      "min          0.000000\n",
      "25%        500.000000\n",
      "50%       1135.000000\n",
      "75%       2636.000000\n",
      "max       3581.000000\n",
      "Name: date_span_days, dtype: float64\n",
      "\n",
      "Note: A ticker with 30 days spread over 2 years is different from 30 consecutive days\n",
      "\n",
      "3. TICKERS BY DATA POINT COUNT:\n",
      "Count of tickers by number of days:\n",
      "num_days\n",
      "1     69\n",
      "2     77\n",
      "3     89\n",
      "4     72\n",
      "5     54\n",
      "6     40\n",
      "7     34\n",
      "8     44\n",
      "9     65\n",
      "10    56\n",
      "11    34\n",
      "12    32\n",
      "13    39\n",
      "14    32\n",
      "15    21\n",
      "16    30\n",
      "17    37\n",
      "18    47\n",
      "19    71\n",
      "20    72\n",
      "21    64\n",
      "22    39\n",
      "23    33\n",
      "24    21\n",
      "25    23\n",
      "26    18\n",
      "27    24\n",
      "28    28\n",
      "29    27\n",
      "30    18\n",
      "31    32\n",
      "32    23\n",
      "33    22\n",
      "34    25\n",
      "35    18\n",
      "36    15\n",
      "37    22\n",
      "38    28\n",
      "39    30\n",
      "40    18\n",
      "41    15\n",
      "42    14\n",
      "43    11\n",
      "44    19\n",
      "45     7\n",
      "46    14\n",
      "47    14\n",
      "48    35\n",
      "49    18\n",
      "50    18\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4. RECOMMENDATION ANALYSIS:\n",
      "======================================================================\n",
      "\n",
      "Threshold | Tickers Removed | % of Total | Rows Removed | % of Rows\n",
      "----------------------------------------------------------------------\n",
      "    10   |       544      |   2.30%  |      2,463   |   0.01%\n",
      "    20   |       943      |   3.99%  |      8,355   |   0.04%\n",
      "    30   |     1,292      |   5.47%  |     16,518   |   0.07%\n",
      "    50   |     1,690      |   7.15%  |     31,991   |   0.14%\n",
      "    60   |     1,845      |   7.81%  |     40,390   |   0.17%\n",
      "    90   |     2,383      |  10.09%  |     80,426   |   0.34%\n",
      "   120   |     2,926      |  12.39%  |    137,460   |   0.59%\n",
      "\n",
      "5. CONSIDERATIONS:\n",
      "   - 30 trading days ≈ 1.5 months (reasonable for basic analysis)\n",
      "   - 60 trading days ≈ 3 months (better for trend analysis)\n",
      "   - 90 trading days ≈ 4.5 months (good for seasonal patterns)\n",
      "   - Also consider: time span (not just count) - consecutive vs spread out\n",
      "   - Many of these short-series tickers are likely:\n",
      "     * Test tickers (ATEST.A, ATEST.B, etc.)\n",
      "     * Warrants/rights (.w, .WS suffixes)\n",
      "     * Preferred shares (.pA, .pB, etc.)\n",
      "     * Which you're already filtering with sanitize_non_equities()\n",
      "\n",
      "6. SUGGESTED APPROACH:\n",
      "   Option A: Filter by count only (simple): 30-60 days\n",
      "   Option B: Filter by count AND time span (more robust):\n",
      "             - At least 30-60 trading days\n",
      "             - AND span at least 3-6 months\n",
      "   Option C: Let other filters handle it (non-equities, low volume)\n",
      "             and only filter extreme cases (< 10-20 days)\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# More comprehensive analysis: Distribution of data points per ticker\n",
    "# This helps determine an informed threshold rather than an arbitrary 30 days\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Calculate stats for each ticker\n",
    "ticker_stats = []\n",
    "\n",
    "for ticker, group in df.groupby('ticker', observed=True):\n",
    "    dates = group['date'].unique()\n",
    "    date_range = (dates.max() - dates.min()).days if len(dates) > 1 else 0\n",
    "    \n",
    "    ticker_stats.append({\n",
    "        'ticker': ticker,\n",
    "        'num_days': len(dates),\n",
    "        'num_rows': len(group),\n",
    "        'date_span_days': date_range,\n",
    "        'start_date': dates.min(),\n",
    "        'end_date': dates.max()\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(ticker_stats)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DISTRIBUTION OF DATA POINTS PER TICKER\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n1. NUMBER OF UNIQUE DAYS PER TICKER:\")\n",
    "print(stats_df['num_days'].describe())\n",
    "print(f\"\\nPercentiles:\")\n",
    "for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]:\n",
    "    val = stats_df['num_days'].quantile(p/100)\n",
    "    count = (stats_df['num_days'] < val).sum()\n",
    "    print(f\"  {p}th percentile: {val:.0f} days ({count:,} tickers have fewer)\")\n",
    "\n",
    "print(\"\\n2. TIME SPAN (CALENDAR DAYS FROM FIRST TO LAST):\")\n",
    "print(stats_df['date_span_days'].describe())\n",
    "print(f\"\\nNote: A ticker with 30 days spread over 2 years is different from 30 consecutive days\")\n",
    "\n",
    "print(\"\\n3. TICKERS BY DATA POINT COUNT:\")\n",
    "print(\"Count of tickers by number of days:\")\n",
    "print(stats_df['num_days'].value_counts().sort_index().head(50))\n",
    "\n",
    "print(\"\\n4. RECOMMENDATION ANALYSIS:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test different thresholds\n",
    "thresholds = [10, 20, 30, 50, 60, 90, 120]\n",
    "print(\"\\nThreshold | Tickers Removed | % of Total | Rows Removed | % of Rows\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "total_tickers = len(stats_df)\n",
    "total_rows = len(df)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    tickers_to_remove = stats_df[stats_df['num_days'] < threshold]['ticker'].tolist()\n",
    "    rows_to_remove = df[df['ticker'].isin(tickers_to_remove)]\n",
    "    \n",
    "    ticker_pct = (len(tickers_to_remove) / total_tickers) * 100\n",
    "    row_pct = (len(rows_to_remove) / total_rows) * 100\n",
    "    \n",
    "    print(f\"   {threshold:3d}   |     {len(tickers_to_remove):5,}      |  {ticker_pct:5.2f}%  |   {len(rows_to_remove):8,}   |  {row_pct:5.2f}%\")\n",
    "\n",
    "print(\"\\n5. CONSIDERATIONS:\")\n",
    "print(\"   - 30 trading days ≈ 1.5 months (reasonable for basic analysis)\")\n",
    "print(\"   - 60 trading days ≈ 3 months (better for trend analysis)\")\n",
    "print(\"   - 90 trading days ≈ 4.5 months (good for seasonal patterns)\")\n",
    "print(\"   - Also consider: time span (not just count) - consecutive vs spread out\")\n",
    "print(\"   - Many of these short-series tickers are likely:\")\n",
    "print(\"     * Test tickers (ATEST.A, ATEST.B, etc.)\")\n",
    "print(\"     * Warrants/rights (.w, .WS suffixes)\")\n",
    "print(\"     * Preferred shares (.pA, .pB, etc.)\")\n",
    "print(\"     * Which you're already filtering with sanitize_non_equities()\")\n",
    "\n",
    "print(\"\\n6. SUGGESTED APPROACH:\")\n",
    "print(\"   Option A: Filter by count only (simple): 30-60 days\")\n",
    "print(\"   Option B: Filter by count AND time span (more robust):\")\n",
    "print(\"             - At least 30-60 trading days\")\n",
    "print(\"             - AND span at least 3-6 months\")\n",
    "print(\"   Option C: Let other filters handle it (non-equities, low volume)\")\n",
    "print(\"             and only filter extreme cases (< 10-20 days)\")\n",
    "\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f3f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1292\n",
      "['AANw', 'AAPGV', 'AAUM', 'AAw', 'ACAw', 'ACCL', 'ACE', 'ACEI', 'ACPrw', 'ACSV', 'ADAT', 'ADEAV', 'ADNTw', 'ADOCU', 'ADSw', 'ADYX', 'ADZ', 'AEBIV', 'AEC', 'AED.CL', 'AEK.CL', 'AESEW', 'AESpC.CL', 'AEXA', 'AFA.CL', 'AFCGV', 'AFGE.CL', 'AFIw', 'AFRU', 'AFW.CL', 'AGCC', 'AGMpB.CL', 'AGRZ', 'AHCOW', 'AHMA', 'AHTpA.CL', 'AHTpE.CL', 'AIFER', 'AIIA.U', 'AIMCV', 'AIVpZ.CL', 'ALH', 'ALISU', 'ALKSV', 'ALLYpB.CL', 'ALLpA.CL', 'ALLpD.CL', 'ALLpE.CL', 'ALLpF.CL', 'ALPpO.CL', 'ALTMW', 'ALVw', 'AMTMw', 'AMUN', 'AMYY', 'ANGIV', 'AOUTV', 'APADR', 'APDw', 'APRB', 'APTVw', 'APVOV', 'APYw', 'AQBTV', 'AREpE.CL', 'ARHpC.CL', 'ARIpA.CL', 'ARIpC.CL', 'ARKIU', 'ARKT', 'ARLOw', 'ARMKw', 'ARMW', 'ARNCw', 'ARRpA.CL', 'ARRpB.CL', 'ARU.CL', 'ASAIw', 'ASCI', 'ASHw', 'ASIXw', 'AST.WSw', 'ASXw', 'ATEST.A', 'ATEST.B', 'ATEST.C', 'ATMUw', 'ATUSw', 'AURE', 'AURU', 'AVKr', 'AVKrw', 'AVOL', 'AVV.CL', 'AVXX', 'AWIw', 'AXAC.U', 'AXARU', 'AXG', 'AXPW', 'AXPWW', 'AXSpD.CL', 'AXUP', 'AZYY', 'BABW', 'BACpY.CL', 'BACpZ.CL', 'BAMw', 'BANCpC.CL', 'BATRR', 'BATRV', 'BBBY.WS', 'BBCPW', 'BBIGV', 'BBTpD.CL', 'BBTpE.CL', 'BBUCw', 'BBUw', 'BBWIw', 'BBYY', 'BCKT', 'BCSFr', 'BCSFrw', 'BCSS.U', 'BCSp.CL', 'BCSpA.CL', 'BCSpC.CL', 'BDAT', 'BDBD', 'BDCI', 'BDCIU', 'BDCIW', 'BDG', 'BDNpE.CL', 'BEPCw', 'BEPw', 'BFLB', 'BFOC', 'BFSpC.CL', 'BGCA.CL', 'BGCPV', 'BGEpB.CL', 'BGIN', 'BGMSP', 'BHFWV', 'BHKr', 'BHKrw', 'BHMw', 'BHVNw', 'BIIBV', 'BIPw', 'BITK', 'BITr', 'BITrw', 'BIVVV', 'BKNU', 'BKTr', 'BKTrw', 'BLSX', 'BMLpI.CL', 'BMNU', 'BNBX', 'BNGOU', 'BNIXU', 'BNKK', 'BNKKW', 'BNw', 'BOCA.CL', 'BRBI', 'BRBRw', 'BRCE', 'BRCM', 'BREM', 'BREZU', 'BRIE', 'BRKRP', 'BRWr', 'BRWrw', 'BSAAR', 'BSMZ', 'BSTr', 'BSTrw', 'BTQ', 'BTTC', 'BTUp', 'BTUw', 'BVNSC', 'BWAw', 'BXRXV', 'CADTU', 'CAFN', 'CAGw', 'CARRw', 'CARSw', 'CBDw', 'CBOL', 'CBOO', 'CBOpA', 'CBTL', 'CBTO', 'CBXL', 'CBXO', 'CCHH', 'CCSIV', 'CCV.CL', 'CDIG', 'CDLRw', 'CEGVV', 'CEIXw', 'CELT', 'CFCpA.CL', 'CFCpB.CL', 'CFRpA.CL', 'CGNTV', 'CGVIC', 'CHECU', 'CHIAV', 'CHIKV', 'CHNB', 'CHNGV', 'CHOW', 'CHPX', 'CHRI', 'CHXw', 'CIMP', 'CISSV', 'CIVEC', 'CLA.CL', 'CLNSpA.CL', 'CLNSpC.CL', 'CLNSpD.CL', 'CLNSpF.CL', 'CLNYpB.CL', 'CLNYpE.CL', 'CLOC', 'CLSX', 'CLV', 'CMDBw', 'CMI.WD', 'CMPOV', 'CMREw', 'CNDTw', 'CNLMU', 'CNQQ', 'CNXCV', 'COFpC.CL', 'COFpD.CL', 'COFpP.CL', 'CONw', 'CORD', 'COSW', 'COTG', 'COTYw', 'CPEpA.CL', 'CRACU', 'CRCD', 'CRCO', 'CRDU', 'CRNCV', 'CRUSC', 'CRw', 'CSANw', 'CSCB', 'CTCT', 'CTEST.A', 'CTEST.B', 'CTEST.C', 'CTEST.D', 'CTEST.E', 'CTEST.F', 'CTEST.G', 'CTEST.H', 'CTEST.I', 'CTEST.J', 'CTEST.K', 'CTEST.L', 'CTEST.M', 'CTEST.N', 'CTEST.O', 'CTEST.P', 'CTEST.Q', 'CTEST.R', 'CTEST.S', 'CTEST.T', 'CTEST.U', 'CTEST.V', 'CTEST.W', 'CTEST.WD', 'CTEST.WS', 'CTEST.X', 'CTEST.Y', 'CTEST.Z', 'CTESTp', 'CTESTr', 'CTESTw', 'CTQ.CL', 'CTRMV', 'CTU.CL', 'CTVAw', 'CTW.CL', 'CTX.CL', 'CURBw', 'CUZw', 'CVB.CL', 'CVETV', 'CVIw', 'CVSL', 'CXTw', 'CYCNV', 'CYHw', 'CpC.CL', 'CpL.CL', 'DASX', 'DBr', 'DBrw', 'DD.WD', 'DDFO', 'DDRw', 'DDTO', 'DDw', 'DEE', 'DELLw', 'DFINw', 'DFSpB.CL', 'DFTpA.CL', 'DFTpB.CL', 'DGOC', 'DHLX', 'DHR.WD', 'DHRw', 'DIME', 'DISHR', 'DISHV', 'DISTU', 'DIVE', 'DKNGZ', 'DKT.CL', 'DLAG', 'DLPHw', 'DLRpE.CL', 'DLRpF.CL', 'DLRpH.CL', 'DMAQU', 'DMIIU', 'DOJE', 'DOM', 'DOVw', 'DOWw', 'DPU', 'DRES', 'DRKY', 'DSXN.CL', 'DTEw', 'DTK.CL', 'DTMw', 'DTZ.CL', 'DUNK', 'DWDPw', 'DXCw', 'DXM', 'DXpA.CL', 'DYAX', 'DYNB', 'EAA.CL', 'EASY', 'EBSw', 'ECACU', 'ECCA.CL', 'ECCZ.CL', 'ECGw', 'ECL.WD', 'EEHB', 'EFM.CL', 'EHABw', 'EHCw', 'EHIr', 'EHIrw', 'EHLDV', 'EHY', 'ELA.CL', 'ELANw', 'ELB.CL', 'ELSpC.CL', 'EMBCV', 'EMBX', 'EMIS', 'EMISR', 'EMISU', 'EMLB', 'EMMAW', 'EMMSP', 'EMOr', 'EMOrw', 'EMQ.CL', 'EMSA', 'EMZ.CL', 'ENHpB.CL', 'ENIAr', 'ENIAw', 'ENICw', 'ENPX', 'EOCAw', 'EOCCw', 'EPMpA.CL', 'EPRpF.CL', 'EQCO.CL', 'EQCpE.CL', 'EQTw', 'ERW', 'ESABw', 'ESEAV', 'ESHAU', 'ESK', 'ESLG', 'ESLV', 'ESSpH.CL', 'ETCw', 'ETHI', 'ETHMU', 'ETMw', 'ETRNw', 'ETTY', 'EVFTC', 'EVLVU', 'EWRI', 'EWRM', 'EWRS', 'EXCVV', 'EXEEW', 'EXTOw', 'EZMO', 'EZRO', 'FATAV', 'FATBV', 'FBYDP', 'FBYY', 'FCRS.U', 'FGDY', 'FGw', 'FIEF', 'FIGG', 'FINSrw', 'FIPWV', 'FLDD', 'FOANC', 'FORw', 'FOXAV', 'FOXBV', 'FPOpA.CL', 'FRCpB.CL', 'FRCpC.CL', 'FRCpD.CL', 'FRMI', 'FTAIV', 'FTDRV', 'FTFrw', 'FTREV', 'FTV.WD', 'FTVw', 'FUSEW', 'FUTG', 'FVEVV', 'FXCOU', 'FXENP', 'GABpD.CL', 'GABrw', 'GBBKU', 'GBDCR', 'GBDCV', 'GCPw', 'GCVpB.CL', 'GCVr', 'GCVrw', 'GDFN', 'GDLC', 'GDLpB.CL', 'GDPpC', 'GDPpD', 'GDVpD.CL', 'GDVr', 'GDVrw', 'GEAR', 'GEB.CL', 'GECXU', 'GEH.CL', 'GEHCV', 'GEK.CL', 'GETVV', 'GEVw', 'GEW', 'GEw', 'GGACU', 'GGO.C', 'GGTpB.CL', 'GGTrw', 'GGZr', 'GGZrw', 'GIPRU', 'GJV.CL', 'GLBAV', 'GLBKV', 'GLIX', 'GLOr', 'GLOrw', 'GLQrw', 'GLUrw', 'GLVrw', 'GME.WS', 'GMOD', 'GNSr', 'GNSrw', 'GOVN', 'GPEpA.CL', 'GPMTw', 'GRALV', 'GRAw', 'GRBIC', 'GRXpA.CL', 'GSJ.CL', 'GSKw', 'GSRF', 'GSRFR', 'GSpI.CL', 'GTECW', 'GTPE', 'GTU', 'GTXw', 'GUA.CL', 'GUTrw', 'GVT', 'GWIIU', 'GXLC', 'GXOw', 'GXPpA.CL', 'GXPpB.CL', 'GXPpD.CL', 'GXPpE.CL', 'GYB.CL', 'HAVAU', 'HBBw', 'HCFTpA.CL', 'HCJ.CL', 'HCNpJ.CL', 'HCOR', 'HCPw', 'HEDG', 'HEGE', 'HEGJ', 'HELI', 'HEpU.CL', 'HFGIC', 'HGVw', 'HHDG', 'HHFR', 'HHHw', 'HIXrw', 'HIYY', 'HLNw', 'HLTw', 'HNRA.U', 'HOMX', 'HONIV', 'HONw', 'HOTw', 'HOYY', 'HPEw', 'HRIw', 'HSEA.CL', 'HSEB.CL', 'HSICV', 'HSONP', 'HTF.CL', 'HTGY.CL', 'HTGZ.CL', 'HTZr', 'HTZrw', 'HTZw', 'HTpB.CL', 'HVMC', 'HVMCW', 'HYP', 'HYTr', 'HYTrw', 'IAAw', 'IACI', 'IACVV', 'IAIC', 'IBACU', 'IBMw', 'ICRC', 'IDHB', 'IDTw', 'IEPRR', 'IETH', 'IFFw', 'IFNr', 'IFNrw', 'IGGY', 'IGRr', 'IGRrw', 'IGU', 'IILGV', 'ILMNV', 'IMPPV', 'INBXV', 'IND.CL', 'INNTW', 'INNpB.CL', 'INNpC.CL', 'INPXV', 'INSWw', 'INXB', 'INXBV', 'INZ.CL', 'IPST', 'IRE', 'IRETp.CL', 'IRETpB.CL', 'IREX', 'IRLr', 'IRLrw', 'ISG.CL', 'ISLEU', 'ISP.CL', 'ISUL', 'ITCBr', 'JANB', 'JBGSw', 'JCIw', 'JEDI', 'JFLX', 'JFRr', 'JFRrw', 'JGHr', 'JGHrw', 'JJEB', 'JMPB.CL', 'JMPC.CL', 'JNJ.WD', 'JOEZ', 'JPMpA.CL', 'JPMpB.CL', 'JPMpD.CL', 'JPMpE.CL', 'JPMpF.CL', 'JQCr', 'JQCrw', 'JULB', 'JWACU', 'JXNw', 'Jw', 'KAP.CL', 'KARw', 'KCAC.WS.A', 'KCC.CL', 'KDK', 'KDKRW', 'KDw', 'KEYpH.CL', 'KFH.CL', 'KFI.CL', 'KFNp.CL', 'KIMpI.CL', 'KIMpJ.CL', 'KIMpK.CL', 'KIOrw', 'KJD', 'KLGw', 'KLXEV', 'KLXIV', 'KNFw', 'KNRX', 'KOYN', 'KOYNW', 'KRCpG.CL', 'KRCpH.CL', 'KRSP.U', 'KSLV', 'KTBw', 'KTUP', 'KVUEw', 'KXINW', 'KYNpE', 'KYNpE.CL', 'KYNpF.CL', 'KYNpG.CL', 'Kw', 'LAACw', 'LACw', 'LAESV', 'LAFAU', 'LATAU', 'LBDAV', 'LBDKV', 'LBMH', 'LBYAV', 'LBYKV', 'LDOSw', 'LEXAV', 'LFS', 'LFTr', 'LFTrw', 'LGLr', 'LGLw', 'LGNDV', 'LGVNR', 'LGVNV', 'LHOpH.CL', 'LHw', 'LIFT', 'LILAR', 'LILRV', 'LKSDw', 'LKSPU', 'LLDM', 'LLEM', 'LLY.WD', 'LMCB', 'LMT.WD', 'LNAI', 'LOTI', 'LOVw', 'LSXMR', 'LSXRV', 'LTHMw', 'LWw', 'LYFX', 'LYLTV', 'MAGC', 'MANI', 'MAXNV', 'MBCw', 'MBVI', 'MBVIW', 'MCK.WD', 'MCLDP', 'MCMJU', 'MCQ.CL', 'MCTA', 'MDA', 'MDAA', 'MDAS', 'MDIAV', 'MDUw', 'MEIL', 'MEILW', 'MEILZ', 'MELA', 'MERpD.CL', 'MERpE.CL', 'MERpF.CL', 'MERpM.CL', 'MERpP.CL', 'METBV', 'METCV', 'METR', 'METw', 'MFGPw', 'MFLA', 'MFSw', 'MGR.CL', 'MHNA.CL', 'MHNB.CL', 'MHOpA.CL', 'MIL', 'MIMO.WS.A', 'MIMO.WS.C', 'MJSC', 'MKLY', 'MKLYR', 'MKTN', 'MLPL', 'MLPV', 'MMID', 'MMM.WD', 'MMMw', 'MMTXU', 'MNRpA.CL', 'MNRpB.CL', 'MPL', 'MPTIw', 'MPpD.CL', 'MRKw', 'MRPw', 'MSCA.CL', 'MSGEw', 'MSGSw', 'MSK.CL', 'MSPRV', 'MSTK', 'MT.WS', 'MT.WSw', 'MTBp.CL', 'MTBpC.CL', 'MTEST.A', 'MTEST.B', 'MTWw', 'MTYY', 'MVCB.CL', 'MW', 'MWG.CL', 'MWO.CL', 'MWR.CL', 'MYCO', 'MYMK', 'NABLw', 'NATLw', 'NBIL', 'NCw', 'NDRAU', 'NEEpC.CL', 'NEEpG.CL', 'NEEpH.CL', 'NELS', 'NEOGV', 'NESRU', 'NETX', 'NFC.Uw', 'NGVTw', 'NHSrw', 'NHYB', 'NIHI', 'NKLR', 'NLOPw', 'NLYpA.CL', 'NLYpC.CL', 'NLYpE.CL', 'NLYpH.CL', 'NMPAR', 'NMRKV', 'NNNpD.CL', 'NNNpE.CL', 'NOKw', 'NOMDw', 'NROr', 'NROrw', 'NSCI', 'NSPRW', 'NTCOw', 'NTEST.A', 'NTEST.B', 'NTEST.C', 'NTEST.D', 'NTEST.E', 'NTEST.F', 'NTEST.P', 'NTEST.R', 'NTEST.S', 'NTEST.T', 'NTEST.U', 'NTEST.V', 'NTEST.W', 'NTEST.WD', 'NTEST.WS', 'NTEST.X', 'NTEST.Y', 'NTEST.Z', 'NTESTp', 'NTESTr', 'NTESTw', 'NTGr', 'NTGrw', 'NTSK', 'NUANV', 'NVACU', 'NVSL', 'NVSTw', 'NVTw', 'NWGw', 'NWMX', 'NXGrw', 'NXUS', 'NYNYR', 'OABIV', 'OACQU', 'OCAT', 'OCTB', 'OEFA', 'OEI', 'OFCpL.CL', 'OGNw', 'OIBRr', 'OIBRrw', 'OKTX', 'ONCHW', 'ONLw', 'ONSIU', 'OPEX', 'OPPrw', 'ORIQ', 'ORO', 'OSCX', 'OSGw', 'OSLE.CL', 'OTISw', 'OVTI', 'OWLS', 'OXSQR', 'OXY.WSw', 'OpF.CL', 'PAII.WS', 'PARRr', 'PARRrw', 'PASW', 'PAYR', 'PBOT', 'PCLG', 'PCP', 'PCR', 'PEBpA', 'PEBpA.CL', 'PEBpB.CL', 'PECKW', 'PEIpA.CL', 'PETZC', 'PFEM', 'PFEw', 'PG.WD', 'PGAC', 'PGACR', 'PGACU', 'PGRI', 'PHINw', 'PHXEp', 'PIOI', 'PJS.CL', 'PKYw', 'PKw', 'PLMr', 'PLMrw', 'PLTS', 'PLYY', 'PLpC.CL', 'PLpE.CL', 'PMCS', 'PMOC', 'PNKZV', 'PNRw', 'PNTA.CL', 'POSTw', 'POZN', 'PQNT', 'PREpD.CL', 'PREpE.CL', 'PRGw', 'PRHI', 'PRHIZ', 'PRSN', 'PRSNW', 'PRSPw', 'PRTHU', 'PRXI', 'PSApA.CL', 'PSApQ.CL', 'PSApR.CL', 'PSApS.CL', 'PSApU.CL', 'PSApV.CL', 'PSApY.CL', 'PSApZ.CL', 'PSBH', 'PSBpS.CL', 'PSBpT.CL', 'PTEST.W', 'PTEST.X', 'PTEST.Y', 'PTEST.Z', 'PTRN', 'PUKw', 'PVA', 'PXED', 'PXIU', 'QBTZ', 'QCCO', 'QCLS', 'QCPw', 'QHCw', 'QLDY', 'QRTAV', 'QRTEV', 'QSU', 'QSX', 'QUMS', 'QUMSR', 'RACEw', 'RALw', 'RBCw', 'RBNEV', 'RBSpE.CL', 'RBSpF.CL', 'RBSpG.CL', 'RBSpH.CL', 'RBSpI.CL', 'RBSpL.CL', 'RBSpR.CL', 'RBSpS.CL', 'RBSpT.CL', 'RBY', 'RCF.U', 'RCLO', 'RDVTV', 'REGpF.CL', 'REGpG.CL', 'REPHV', 'REPR', 'REZIw', 'RGTZ', 'RHLDV', 'RIFr', 'RIFrw', 'RITMpE', 'RIVrw', 'RJD.CL', 'RJDI', 'RJMI', 'RJVI', 'RLGTpA.CL', 'RMNIU', 'RMNIW', 'RNGTU', 'RNRpC.CL', 'ROLA', 'ROYL', 'RPAYW', 'RPGL', 'RPIBC', 'RPRXW', 'RPRXZ', 'RPX', 'RQIr', 'RQIrw', 'RRDw', 'RRTSr', 'RSFr', 'RSFrw', 'RSOpA.CL', 'RSOpB.CL', 'RSTN', 'RTLA', 'RVIw', 'RWG', 'RXNw', 'RXOw', 'SAB.CL', 'SANpA.CL', 'SANpI.CL', 'SAQ.CL', 'SATSV', 'SBEU', 'SBGLr', 'SBGLrw', 'SBTU', 'SBpB.CL', 'SCDr', 'SCDrw', 'SCEpF.CL', 'SCHWpB.CL', 'SCQ.CL', 'SEGr', 'SEGrw', 'SEGw', 'SEMw', 'SFLA', 'SFN.CL', 'SFXE', 'SGBXV', 'SGDVV', 'SGYPU', 'SGZA.CL', 'SHELw', 'SHOpD.CL', 'SIFYR', 'SITCpJ.CL', 'SITCw', 'SJOYW', 'SLAI', 'SLGB', 'SLMT', 'SLNDW', 'SLOW', 'SLRA.CL', 'SLTB.CL', 'SLVMw', 'SMSHW', 'SMTAw', 'SMYY', 'SNDKV', 'SNREV', 'SNVpC.CL', 'SOCAW', 'SOLSV', 'SOLVw', 'SOVpC.CL', 'SPCT', 'SPDC', 'SPErw', 'SPHA', 'SPHAR', 'SPHRw', 'SPIT', 'SPLZ', 'SPTU', 'SPWRV', 'SQMr', 'SQMrw', 'SRCw', 'SRPU', 'SRVrw', 'SSEA', 'SSEAR', 'SSWpC.CL', 'STAGpA.CL', 'STAGpB.CL', 'STEN', 'STHOV', 'STNEV', 'STRDW', 'STTpC.CL', 'STTpE.CL', 'STUB', 'SUNSV', 'SWAY', 'SWBIV', 'SWIw', 'SWJ.CL', 'SWSH', 'SYA', 'SZCrw', 'T.WD', 'TAKw', 'TALL', 'TAS', 'TAXIL', 'TBXU', 'TCCA.CL', 'TCCB.CL', 'TCFpB.CL', 'TCFpC.CL', 'TCGL', 'TCPA', 'TCRX.CL', 'TEND', 'TENJ', 'TENM', 'TEXU', 'TFCF', 'TFCFA', 'TGNAw', 'THRV', 'THTI', 'TIGOR', 'TIMBw', 'TINS', 'TLNC', 'TLNCW', 'TLR', 'TMKpB.CL', 'TMSRW', 'TMUSR', 'TNPpB.CL', 'TNT', 'TOCT', 'TOROV', 'TPVZ.CL', 'TRBF', 'TRC.WS', 'TRCr', 'TRCrw', 'TRIV', 'TRLA', 'TRNOpA.CL', 'TRNw', 'TSLV', 'TSVTV', 'TSXD', 'TSXU', 'TTDU', 'TTRX', 'TTXD', 'TTXU', 'TWNPV', 'TWOw', 'TXAC', 'TYDEV', 'TYGpB', 'TYGpB.CL', 'TYGpC.CL', 'TZF', 'TZF.CL', 'UA.Cw', 'UBEW', 'UBPpF.CL', 'UBPpG.CL', 'UBSpD.CL', 'UCFI', 'UCFIW', 'UHALB', 'UMHpA.CL', 'UNX', 'USBpN.CL', 'UTFr', 'UTFrw', 'UTGr', 'UTGrw', 'UTIW', 'UTLZ', 'UTXw', 'VALEw', 'VANIW', 'VAPEW', 'VATEr', 'VEXC', 'VFCw', 'VGGL', 'VGHY', 'VINCU', 'VIVw', 'VLLV', 'VLML', 'VLTOw', 'VLYWW', 'VMEOV', 'VMWw', 'VNEw', 'VNOpG.CL', 'VNOpI.CL', 'VNOpJ.CL', 'VNOw', 'VNTw', 'VPCO', 'VPCOU', 'VREXV', 'VRpA.CL', 'VRpB.CL', 'VSCOw', 'VSMw', 'VSTSw', 'VTRSV', 'VTSw', 'VVVw', 'VYXw', 'VZA.CL', 'WABw', 'WAVX', 'WBDWV', 'WBI', 'WBS.WS', 'WBSpE.CL', 'WDCVV', 'WFEpA.CL', 'WFw', 'WHZ', 'WHw', 'WINR', 'WINRW', 'WIZE', 'WKEYV', 'WNRw', 'WORw', 'WPCw', 'WQTM', 'WRKw', 'WSBCO', 'WSH', 'WSw', 'WTRHW', 'WULX', 'WYNDw', 'XBITV', 'XELLL', 'XLYO', 'XPERw', 'XPOw', 'XPVVV', 'XRPR', 'XRXw', 'YCY.U', 'YDDL', 'YDKG', 'YSAC.U', 'YSAC.WS', 'YUMCw', 'YUMw', 'YYJ.TEST', 'ZDGEw', 'ZDVSV', 'ZIMVV', 'ZINC', 'ZLSWU', 'ZMUN', 'ZVSAW', 'ZVZZC']\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of tickers that have very little continuous trading data and determine a threshold to filter out, perhaps less than 30 days\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df['date'] = pd.to_datetime(df['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "\n",
    "tickers_too_little_data = []\n",
    "\n",
    "for ticker, group in df.groupby('ticker', observed=True):\n",
    "    dates = group['date'].unique()\n",
    "\n",
    "    if len(dates) < 30:\n",
    "        tickers_too_little_data.append(ticker)\n",
    "\n",
    "print(len(tickers_too_little_data))\n",
    "print(tickers_too_little_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8017a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze types in ticker column\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TICKER COLUMN TYPE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Column dtype\n",
    "print(f\"\\nColumn dtype: {df['ticker'].dtype}\")\n",
    "\n",
    "# Count Python types\n",
    "type_counts = Counter(type(val).__name__ for val in df['ticker'])\n",
    "print(f\"\\nPython type distribution:\")\n",
    "for py_type, count in type_counts.most_common():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"  {py_type}: {count:,} rows ({percentage:.2f}%)\")\n",
    "\n",
    "# NaN analysis\n",
    "nan_count = df['ticker'].isna().sum()\n",
    "print(f\"\\nNaN values: {nan_count:,} rows ({(nan_count/len(df)*100):.2f}%)\")\n",
    "\n",
    "# Sample values by type\n",
    "print(f\"\\nSample values by type:\")\n",
    "for py_type in type_counts.keys():\n",
    "    sample_values = [val for val in df['ticker'] if type(val).__name__ == py_type][:5]\n",
    "    print(f\"  {py_type}: {sample_values}\")\n",
    "\n",
    "# Check for numeric tickers (might be read as float)\n",
    "numeric_tickers = df[pd.to_numeric(df['ticker'], errors='coerce').notna()]\n",
    "if len(numeric_tickers) > 0:\n",
    "    print(f\"\\n⚠️  Found {len(numeric_tickers)} rows with numeric ticker values:\")\n",
    "    print(f\"   Sample: {numeric_tickers['ticker'].head(10).tolist()}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total rows: {len(df):,}\")\n",
    "print(f\"String tickers: {type_counts.get('str', 0):,}\")\n",
    "print(f\"Non-string tickers: {sum(count for type_name, count in type_counts.items() if type_name != 'str'):,}\")\n",
    "print(f\"NaN tickers: {nan_count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b81b6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect rows with NaN ticker values\n",
    "import pandas as pd\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date\n",
    "df['date'] = pd.to_datetime(df['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Find rows with NaN tickers\n",
    "nan_ticker_rows = df[df['ticker'].isna()]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ROWS WITH NaN TICKER VALUES\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total rows with NaN ticker: {len(nan_ticker_rows)}\")\n",
    "print(f\"\\nSample of rows with NaN ticker (first 20):\")\n",
    "print(nan_ticker_rows.head(20))\n",
    "\n",
    "print(f\"\\n\\nDate range for NaN ticker rows:\")\n",
    "print(f\"  From: {nan_ticker_rows['date'].min()}\")\n",
    "print(f\"  To: {nan_ticker_rows['date'].max()}\")\n",
    "\n",
    "print(f\"\\n\\nOther columns with NaN values in these rows:\")\n",
    "for col in nan_ticker_rows.columns:\n",
    "    nan_count = nan_ticker_rows[col].isna().sum()\n",
    "    if nan_count > 0:\n",
    "        print(f\"  {col}: {nan_count} NaN values ({nan_count/len(nan_ticker_rows)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n\\nVolume statistics for NaN ticker rows:\")\n",
    "print(nan_ticker_rows['volume'].describe())\n",
    "\n",
    "print(f\"\\n\\nPrice statistics for NaN ticker rows:\")\n",
    "print(nan_ticker_rows[['open', 'close', 'high', 'low']].describe())\n",
    "\n",
    "print(f\"\\n\\nUnique dates with NaN tickers: {nan_ticker_rows['date'].nunique()}\")\n",
    "print(f\"Date distribution (top 10 dates with most NaN tickers):\")\n",
    "print(nan_ticker_rows['date'].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d531882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Test: String (object) vs Category dtype for ticker column\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df_test = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date\n",
    "df_test['date'] = pd.to_datetime(df_test['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in df_test.columns if col != 'date']\n",
    "df_test = df_test[cols]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET INFO\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total rows: {len(df_test):,}\")\n",
    "print(f\"Unique tickers: {df_test['ticker'].nunique():,}\")\n",
    "print(f\"Average rows per ticker: {len(df_test) / df_test['ticker'].nunique():.1f}\")\n",
    "print(f\"\\nMemory usage (object dtype):\")\n",
    "mem_obj = df_test['ticker'].memory_usage(deep=True) / 1024**2\n",
    "print(f\"{mem_obj:.2f} MB\")\n",
    "\n",
    "# Create a copy with category dtype\n",
    "df_cat = df_test.copy()\n",
    "df_cat['ticker'] = df_cat['ticker'].astype('category')\n",
    "\n",
    "print(f\"\\nMemory usage (category dtype):\")\n",
    "mem_cat = df_cat['ticker'].memory_usage(deep=True) / 1024**2\n",
    "print(f\"{mem_cat:.2f} MB\")\n",
    "print(f\"Memory savings: {(1 - mem_cat / mem_obj) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE TESTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test 1: groupby operations\n",
    "print(\"\\n1. GROUPBY OPERATIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "result_obj = df_test.groupby('ticker')['volume'].sum()\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "result_cat = df_cat.groupby('ticker')['volume'].sum()\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 2: Filtering with isin()\n",
    "print(\"\\n2. FILTERING WITH isin()\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Get a sample of tickers to filter\n",
    "sample_tickers = df_test['ticker'].unique()[:1000].tolist()\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "filtered_obj = df_test[df_test['ticker'].isin(sample_tickers)]\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "filtered_cat = df_cat[df_cat['ticker'].isin(sample_tickers)]\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 3: Sorting\n",
    "print(\"\\n3. SORTING\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "sorted_obj = df_test.sort_values('ticker')\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "sorted_cat = df_cat.sort_values('ticker')\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 4: value_counts()\n",
    "print(\"\\n4. value_counts()\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "vc_obj = df_test['ticker'].value_counts()\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "vc_cat = df_cat['ticker'].value_counts()\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 5: set_index with ticker\n",
    "print(\"\\n5. set_index() with ticker\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "idx_obj = df_test.set_index(['ticker', 'date'])\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "idx_cat = df_cat.set_index(['ticker', 'date'])\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")\n",
    "\n",
    "# Test 6: Aggregation operations\n",
    "print(\"\\n6. MULTIPLE AGGREGATIONS\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Object dtype\n",
    "start = time.time()\n",
    "agg_obj = df_test.groupby('ticker').agg({\n",
    "    'volume': ['sum', 'mean', 'max'],\n",
    "    'close': ['mean', 'std']\n",
    "})\n",
    "time_obj = time.time() - start\n",
    "print(f\"Object dtype: {time_obj:.4f} seconds\")\n",
    "\n",
    "# Category dtype\n",
    "start = time.time()\n",
    "agg_cat = df_cat.groupby('ticker').agg({\n",
    "    'volume': ['sum', 'mean', 'max'],\n",
    "    'close': ['mean', 'std']\n",
    "})\n",
    "time_cat = time.time() - start\n",
    "print(f\"Category dtype: {time_cat:.4f} seconds\")\n",
    "print(f\"Speedup: {time_obj / time_cat:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815ba263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing how much of the data (or haw many ticker series) have at MAX 100 shares vol traded on a day through their ENTIRE series \n",
    "# and never reached above $0.01 on their ENTIRE series. \n",
    "\n",
    "# Goal is to potentially remove some untradeable noise, but first to see how much of that noise we would be removing\n",
    "\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "unique_tickers = OHLCV_data['ticker'].unique()\n",
    "\n",
    "# Find and analyze the number of tickers that have MAX 100 shares volume across their entire series \n",
    "# Compute max volume and max price per ticker in one pass\n",
    "ticker_stats = OHLCV_data.groupby('ticker').agg({\n",
    "    'volume': 'max',\n",
    "    'close': 'max'\n",
    "}).reset_index()\n",
    "\n",
    "# See what the distribution of price and volume is across our series\n",
    "print(ticker_stats['volume'].min())\n",
    "print(ticker_stats['volume'].quantile([0.01, 0.05, 0.10, 0.25, 0.50]))\n",
    "print(ticker_stats['close'].min())\n",
    "\n",
    "# Filter tickers into a list that are below the criteria\n",
    "invalid_tickers = ticker_stats[\n",
    "    (ticker_stats['volume'] < 1000) |\n",
    "    (ticker_stats['close'] < 0.01)\n",
    "]['ticker'].tolist()\n",
    "\n",
    "print(invalid_tickers)\n",
    "print(len(invalid_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0999a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at all of the suffixes we have for the unique tickers\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "# Get unique tickers and filter out NaN values, convert to strings\n",
    "unique_tickers = OHLCV_data['ticker'].dropna().unique()\n",
    "unique_tickers = [str(ticker) for ticker in unique_tickers]\n",
    "\n",
    "# Function to extract suffix from a ticker\n",
    "def extract_suffix(ticker):\n",
    "    \"\"\"\n",
    "    Extract suffix from ticker. Returns (base, suffix) or (ticker, None) if no suffix detected.\n",
    "    Suffixes are typically 1-3 characters at the end that are not digits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Match test tickers (case-insensitive)\n",
    "    if re.search(r'(?i)test', ticker):\n",
    "        # Handle test tickers\n",
    "        return ticker, 'TEST'\n",
    "\n",
    "    # Match ZVZZT/ZWZZT test tickers\n",
    "    if re.match(r'^(ZVZZT|ZWZZT)$', ticker):\n",
    "        return ticker, None\n",
    "\n",
    "    # Match non-equities (lowercase/period suffixes)\n",
    "    non_equities_match = re.match(r'^([^a-z.]*)([a-z.].*)$', ticker)\n",
    "    if non_equities_match:\n",
    "        base, suffix = non_equities_match.groups()\n",
    "        if len(base) >= 1 and len(suffix) >= 1:\n",
    "            return base, suffix\n",
    "\n",
    "    # If no pattern matches, return the whole ticker as base with no suffix\n",
    "    return ticker, None\n",
    "\n",
    "# Group tickers by suffix - simple dictionary mapping suffix -> list of tickers\n",
    "suffix_groups = defaultdict(list)\n",
    "\n",
    "for ticker in unique_tickers:\n",
    "    base, suffix = extract_suffix(ticker)\n",
    "    if suffix:\n",
    "        suffix_groups[suffix].append(ticker)\n",
    "    else:\n",
    "        # Tickers with no detected suffix go under a no_suffix column\n",
    "        suffix_groups['no suffix'].append(ticker)\n",
    "\n",
    "\n",
    "# Find the maximum length needed for DataFrame\n",
    "max_len = max(len(tickers) for tickers in suffix_groups.values()) if suffix_groups else 0\n",
    "\n",
    "# Pad all lists to the same length with NaN\n",
    "for suffix in suffix_groups:\n",
    "    suffix_groups[suffix] = suffix_groups[suffix] + [np.nan] * (max_len - len(suffix_groups[suffix]))\n",
    "\n",
    "# Create DataFrame\n",
    "suffix_df = pd.DataFrame(dict(sorted(suffix_groups.items())))\n",
    "\n",
    "# take a look at the dataframe\n",
    "print(suffix_df)\n",
    "\n",
    "# Dictionary to store the column names (suffixes) and the COUNT of tickers that fall under that suffix \n",
    "# we dont currently use the dictionary but could be nice to have\n",
    "suffix_dict = {}\n",
    "\n",
    "for column_name, column_data in suffix_df.items():\n",
    "    suffix_dict[column_name] = column_data.count()\n",
    "    print(f\"{column_name} : {column_data.count()}\")\n",
    "\n",
    "# number of non suffixed unique tickers\n",
    "count_no_suffix = suffix_dict['no suffix']\n",
    "# number of uniquely suffix tickers identified by the regex\n",
    "count_suffix = len(unique_tickers) - count_no_suffix\n",
    "\n",
    "# Percent of uniquely suffixed tickers for all unique tickers \n",
    "# Will be removinig these \n",
    "print(count_suffix / len(unique_tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ceb8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing duplicates\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in OHLCV Data \n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "OHLCV_data = pd.read_csv(csv_path)\n",
    "\n",
    "# Convert to pandas datetime and normalize to date (removes time component)\n",
    "# Keeping as pandas datetime (not Python date) for pandas operations like .dt.year\n",
    "OHLCV_data['date'] = pd.to_datetime(OHLCV_data['window_start'], unit='ns').dt.normalize()\n",
    "\n",
    "# Reorder columns to put 'date' first\n",
    "cols = ['date'] + [col for col in OHLCV_data.columns if col != 'date']\n",
    "OHLCV_data[cols]\n",
    "\n",
    "# Find the duplcate rows\n",
    "duplicate_rows = OHLCV_data[OHLCV_data.duplicated(subset=['ticker', 'date'], keep=False)]\n",
    "print(type(duplicate_rows))\n",
    "\n",
    "# Select only 'ticker' and 'date' columns, sort by ticker\n",
    "dupes = duplicate_rows[['ticker', 'date', 'close','open', 'volume']].sort_values(by='ticker')\n",
    "\n",
    "print(dupes)\n",
    "print(len(dupes))\n",
    "# print(len(duplicate_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78ccd951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe before cleaning:\n",
      "================================\n",
      "Count of Unique Tickers: 23622\n",
      "Number of Rows: 23408074\n",
      "================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\carso\\Development\\emerytrading\\Polygon\\cleaning.py:166: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, 'date'] = pd.to_datetime(df.loc[:, 'window_start'], unit='ns').dt.normalize()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe after cleaning:\n",
      "================================\n",
      "Count of Unique Tickers: 20827\n",
      "Number of Rows: 21967114\n",
      "================================\n",
      "======================================================================\n",
      "API CALLS SUMMARY\n",
      "======================================================================\n",
      "Total API calls needed: 3,488,466\n",
      "Unique tickers: 20,827\n",
      "Total continuous series: 20,827\n",
      "Average calls per ticker: 167.50\n",
      "Average calls per series: 167.50\n",
      "\n",
      "Distribution of API calls per series:\n",
      "count    20827.000000\n",
      "mean       167.497287\n",
      "std        128.056701\n",
      "min          1.000000\n",
      "25%         58.000000\n",
      "50%        128.000000\n",
      "75%        294.000000\n",
      "max        366.000000\n",
      "Name: api_calls_needed, dtype: float64\n",
      "\n",
      "Top 10 tickers by API calls needed:\n",
      "  AADR: 366 calls\n",
      "  AAL: 366 calls\n",
      "  A: 366 calls\n",
      "  AA: 366 calls\n",
      "  AAP: 366 calls\n",
      "  AAON: 366 calls\n",
      "  AAOI: 366 calls\n",
      "  AAME: 366 calls\n",
      "  GDV: 366 calls\n",
      "  GDO: 366 calls\n",
      "\n",
      "Series with most API calls needed:\n",
      "ticker series_start series_end  days_in_range  api_calls_needed\n",
      "     A   2016-01-04 2025-10-24           2560               366\n",
      "    AA   2016-01-04 2025-10-24           2560               366\n",
      "  AADR   2016-01-04 2025-10-24           2560               366\n",
      "   AAL   2016-01-04 2025-10-24           2560               366\n",
      "  AAME   2016-01-04 2025-10-24           2560               366\n",
      "  AAOI   2016-01-04 2025-10-24           2560               366\n",
      "  AAON   2016-01-04 2025-10-24           2560               366\n",
      "   AAP   2016-01-04 2025-10-24           2560               366\n",
      "  AAPL   2016-01-04 2025-10-24           2560               366\n",
      "   AAT   2016-01-04 2025-10-24           2560               366\n",
      "======================================================================\n",
      "======================================================================\n",
      "API CALLS SUMMARY\n",
      "======================================================================\n",
      "Total API calls needed: 1,748,438\n",
      "Unique tickers: 20,827\n",
      "Total continuous series: 20,827\n",
      "Average calls per ticker: 83.95\n",
      "Average calls per series: 83.95\n",
      "\n",
      "Distribution of API calls per series:\n",
      "count    20827.000000\n",
      "mean        83.950545\n",
      "std         63.952070\n",
      "min          1.000000\n",
      "25%         29.000000\n",
      "50%         64.000000\n",
      "75%        147.000000\n",
      "max        183.000000\n",
      "Name: api_calls_needed, dtype: float64\n",
      "\n",
      "Top 10 tickers by API calls needed:\n",
      "  AADR: 183 calls\n",
      "  AAL: 183 calls\n",
      "  A: 183 calls\n",
      "  AA: 183 calls\n",
      "  AAP: 183 calls\n",
      "  AAON: 183 calls\n",
      "  AAOI: 183 calls\n",
      "  AAME: 183 calls\n",
      "  AAPL: 183 calls\n",
      "  GEL: 183 calls\n",
      "\n",
      "Series with most API calls needed:\n",
      "ticker series_start series_end  days_in_range  api_calls_needed\n",
      "     A   2016-01-04 2025-10-24           2560               183\n",
      "    AA   2016-01-04 2025-10-24           2560               183\n",
      "  AADR   2016-01-04 2025-10-24           2560               183\n",
      "   AAL   2016-01-04 2025-10-24           2560               183\n",
      "  AAME   2016-01-04 2025-10-24           2560               183\n",
      "  AAOI   2016-01-04 2025-10-24           2560               183\n",
      "  AAON   2016-01-04 2025-10-24           2560               183\n",
      "   AAP   2016-01-04 2025-10-24           2560               183\n",
      "  AAPL   2016-01-04 2025-10-24           2560               183\n",
      "   AAT   2016-01-04 2025-10-24           2560               183\n",
      "======================================================================\n",
      "======================================================================\n",
      "API CALLS SUMMARY\n",
      "======================================================================\n",
      "Total API calls needed: 822,966\n",
      "Unique tickers: 20,827\n",
      "Total continuous series: 20,827\n",
      "Average calls per ticker: 39.51\n",
      "Average calls per series: 39.51\n",
      "\n",
      "Distribution of API calls per series:\n",
      "count    20827.000000\n",
      "mean        39.514380\n",
      "std         29.942477\n",
      "min          1.000000\n",
      "25%         14.000000\n",
      "50%         30.000000\n",
      "75%         69.000000\n",
      "max         86.000000\n",
      "Name: api_calls_needed, dtype: float64\n",
      "\n",
      "Top 10 tickers by API calls needed:\n",
      "  AADR: 86 calls\n",
      "  AAL: 86 calls\n",
      "  A: 86 calls\n",
      "  AA: 86 calls\n",
      "  AAP: 86 calls\n",
      "  AAON: 86 calls\n",
      "  AAOI: 86 calls\n",
      "  AAME: 86 calls\n",
      "  AAPL: 86 calls\n",
      "  ZSL: 86 calls\n",
      "\n",
      "Series with most API calls needed:\n",
      "ticker series_start series_end  days_in_range  api_calls_needed\n",
      "     A   2016-01-04 2025-10-24           2560                86\n",
      "    AA   2016-01-04 2025-10-24           2560                86\n",
      "  AADR   2016-01-04 2025-10-24           2560                86\n",
      "   AAL   2016-01-04 2025-10-24           2560                86\n",
      "  AAME   2016-01-04 2025-10-24           2560                86\n",
      "  AAOI   2016-01-04 2025-10-24           2560                86\n",
      "  AAON   2016-01-04 2025-10-24           2560                86\n",
      "   AAP   2016-01-04 2025-10-24           2560                86\n",
      "  AAPL   2016-01-04 2025-10-24           2560                86\n",
      "   AAT   2016-01-04 2025-10-24           2560                86\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate API calls needed for comprehensive data collection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "\n",
    "from cleaning import run_cleaning\n",
    "\n",
    "def calculate_api_calls_needed(\n",
    "    df: pd.DataFrame,\n",
    "    frequency_days: int = 1,\n",
    "    use_trading_days: bool = True,\n",
    "    gap_threshold_days: int = None,\n",
    "    ticker_col: str = 'ticker',  # Parameter: defaults to 'ticker' - the name of your ticker column\n",
    "    date_col: str = 'date'        # Parameter: defaults to 'date' - the name of your date column\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the number of API calls needed to collect comprehensive data for each ticker's continuous series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with ticker and date columns (or whatever you name them)\n",
    "    frequency_days : int, default=1\n",
    "        How often to call the API (1=daily, 7=weekly, 30=monthly, etc.)\n",
    "    use_trading_days : bool, default=True\n",
    "        If True, uses business days (excludes weekends/holidays). If False, uses calendar days.\n",
    "    gap_threshold_days : int, optional\n",
    "        If provided, splits ticker series at gaps larger than this threshold (handles ticker reuse).\n",
    "        If None, treats each ticker as a single continuous series from min to max date.\n",
    "    ticker_col : str, default='ticker'\n",
    "        The name of the column containing ticker symbols in your DataFrame.\n",
    "        Since your column is named 'ticker', you can ignore this parameter.\n",
    "    date_col : str, default='date'\n",
    "        The name of the column containing dates in your DataFrame.\n",
    "        Since your column is named 'date', you can ignore this parameter.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with columns:\n",
    "        - ticker: Ticker symbol\n",
    "        - series_start: Start date of the continuous series\n",
    "        - series_end: End date of the continuous series\n",
    "        - days_in_range: Number of days (trading or calendar) in the range\n",
    "        - api_calls_needed: Number of API calls needed for this series\n",
    "    \"\"\"\n",
    "    \n",
    "    # ticker_col and date_col are parameters (variables) that hold the column names\n",
    "    # When you call df[ticker_col], it's the same as df['ticker'] (since ticker_col = 'ticker' by default)\n",
    "    # This makes the function flexible - if someone's columns were named differently, they could pass different names\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    # df[date_col] means: access the column whose name is stored in the date_col variable\n",
    "    # So df[date_col] = df['date'] when date_col='date' (the default)\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "        df = df.copy()\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Sort by ticker and date for gap detection\n",
    "    # [ticker_col, date_col] creates a list like ['ticker', 'date'] to sort by both columns\n",
    "    df_sorted = df.sort_values([ticker_col, date_col]).reset_index(drop=True)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Group by ticker\n",
    "    # df_sorted.groupby(ticker_col) is the same as df_sorted.groupby('ticker') when ticker_col='ticker'\n",
    "    for ticker, group in df_sorted.groupby(ticker_col, observed=True):\n",
    "        # group[date_col] accesses the date column from this ticker's group\n",
    "        group = group.sort_values(date_col).reset_index(drop=True)\n",
    "        dates = group[date_col].unique()\n",
    "        \n",
    "        if len(dates) < 2:\n",
    "            # Single date or no dates - still need at least 1 API call\n",
    "            series_start = dates[0] if len(dates) == 1 else None\n",
    "            series_end = series_start\n",
    "            \n",
    "            if series_start is not None:\n",
    "                if use_trading_days:\n",
    "                    days_in_range = 1\n",
    "                else:\n",
    "                    days_in_range = 1\n",
    "                \n",
    "                api_calls = max(1, ceil(days_in_range / frequency_days))\n",
    "                \n",
    "                results.append({\n",
    "                    'ticker': ticker,\n",
    "                    'series_start': series_start,\n",
    "                    'series_end': series_end,\n",
    "                    'days_in_range': days_in_range,\n",
    "                    'api_calls_needed': api_calls\n",
    "                })\n",
    "            continue\n",
    "        \n",
    "        # Detect continuous series\n",
    "        if gap_threshold_days is not None:\n",
    "            # Split into continuous series based on gaps\n",
    "            series_list = []\n",
    "            current_series_start = dates[0]\n",
    "            \n",
    "            for i in range(1, len(dates)):\n",
    "                gap = (dates[i] - dates[i-1]).days\n",
    "                \n",
    "                if gap > gap_threshold_days:\n",
    "                    # End current series, start new one\n",
    "                    series_list.append((current_series_start, dates[i-1]))\n",
    "                    current_series_start = dates[i]\n",
    "            \n",
    "            # Add the last series\n",
    "            series_list.append((current_series_start, dates[-1]))\n",
    "        else:\n",
    "            # Treat as single continuous series\n",
    "            series_list = [(dates[0], dates[-1])]\n",
    "        \n",
    "        # Calculate API calls for each continuous series\n",
    "        for series_start, series_end in series_list:\n",
    "            if use_trading_days:\n",
    "                # Use business days (excludes weekends, but not holidays)\n",
    "                date_range = pd.bdate_range(start=series_start, end=series_end, inclusive='both')\n",
    "                days_in_range = len(date_range)\n",
    "            else:\n",
    "                # Use calendar days\n",
    "                days_in_range = (series_end - series_start).days + 1\n",
    "            \n",
    "            # Calculate API calls needed (always at least 1, round up)\n",
    "            api_calls = max(1, ceil(days_in_range / frequency_days))\n",
    "            \n",
    "            results.append({\n",
    "                'ticker': ticker,\n",
    "                'series_start': series_start,\n",
    "                'series_end': series_end,\n",
    "                'days_in_range': days_in_range,\n",
    "                'api_calls_needed': api_calls\n",
    "            })\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Convert date columns to datetime if they exist\n",
    "    if len(result_df) > 0:\n",
    "        result_df['series_start'] = pd.to_datetime(result_df['series_start'])\n",
    "        result_df['series_end'] = pd.to_datetime(result_df['series_end'])\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def summarize_api_calls(api_calls_df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Print a summary of API calls needed.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    api_calls_df : pd.DataFrame\n",
    "        DataFrame returned from calculate_api_calls_needed()\n",
    "    \"\"\"\n",
    "    if len(api_calls_df) == 0:\n",
    "        print(\"No data to summarize.\")\n",
    "        return\n",
    "    \n",
    "    total_calls = api_calls_df['api_calls_needed'].sum()\n",
    "    unique_tickers = api_calls_df['ticker'].nunique()\n",
    "    total_series = len(api_calls_df)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"API CALLS SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total API calls needed: {total_calls:,}\")\n",
    "    print(f\"Unique tickers: {unique_tickers:,}\")\n",
    "    print(f\"Total continuous series: {total_series:,}\")\n",
    "    print(f\"Average calls per ticker: {total_calls / unique_tickers:.2f}\")\n",
    "    print(f\"Average calls per series: {total_calls / total_series:.2f}\")\n",
    "    print()\n",
    "    print(\"Distribution of API calls per series:\")\n",
    "    print(api_calls_df['api_calls_needed'].describe())\n",
    "    print()\n",
    "    print(\"Top 10 tickers by API calls needed:\")\n",
    "    top_tickers = api_calls_df.groupby('ticker')['api_calls_needed'].sum().sort_values(ascending=False).head(10)\n",
    "    for ticker, calls in top_tickers.items():\n",
    "        print(f\"  {ticker}: {calls:,} calls\")\n",
    "    print()\n",
    "    print(\"Series with most API calls needed:\")\n",
    "    top_series = api_calls_df.nlargest(10, 'api_calls_needed')[['ticker', 'series_start', 'series_end', 'days_in_range', 'api_calls_needed']]\n",
    "    print(top_series.to_string(index=False))\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "\n",
    "\n",
    "csv_path = r'C:\\Users\\carso\\Development\\emerytrading\\Data\\Stocks\\Polygon\\OHLCV_Historical_2016-01-01_to_2025-10-26.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "df = run_cleaning(df)\n",
    "\n",
    "\n",
    "# Calculate with different scenarios:\n",
    "# 1. Daily calls, no gap detection (treat each ticker as single series)\n",
    "api_calls_daily = calculate_api_calls_needed(df, frequency_days=7, use_trading_days=True, gap_threshold_days=None)\n",
    "summarize_api_calls(api_calls_daily)\n",
    "\n",
    "# 2. Weekly calls, with gap detection (split at gaps > 365 days)\n",
    "api_calls_weekly = calculate_api_calls_needed(df, frequency_days=14, use_trading_days=True, gap_threshold_days=None)\n",
    "summarize_api_calls(api_calls_weekly)\n",
    "\n",
    "# 3. Monthly calls, with gap detection\n",
    "api_calls_monthly = calculate_api_calls_needed(df, frequency_days=30, use_trading_days=True, gap_threshold_days=None)\n",
    "summarize_api_calls(api_calls_monthly)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (emerytrading)",
   "language": "python",
   "name": "emerytrading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
